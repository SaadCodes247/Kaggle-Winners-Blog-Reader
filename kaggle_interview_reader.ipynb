{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74839b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8f48a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\Sahm9\\Work\\Github\\projects\\kaggle_interviews_reader\\WinnersInterviewBlogPosts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3b268cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Computer scientist Jure Zbontar on winning the...</td>\n",
       "      <td>http://blog.kaggle.com/2010/06/09/computer-sci...</td>\n",
       "      <td>2010-06-09 18:22:29</td>\n",
       "      <td>My approach was actually quite simple. The onl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Computer scientist Jure Zbontar on winning the...   \n",
       "\n",
       "                                                link     publication_date  \\\n",
       "0  http://blog.kaggle.com/2010/06/09/computer-sci...  2010-06-09 18:22:29   \n",
       "\n",
       "                                             content  \n",
       "0  My approach was actually quite simple. The onl...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ab0ab4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My approach was actually quite simple. The only attributes I used where the approximate betting odds and the information on past voting. I sought patterns in the voting behaviour of all countries and combined that knowledge with this year's betting odds. I used cross-validation to select my model and to avoid overfitting it.\n",
      "Predicting the finalists \n",
      "\n",
      "I trusted the bookmakers on this one and just took the top ten countries from each semi-final group. I got the betting odds from Betfair.\n",
      "\n",
      "Learning the voting patterns \n",
      "\n",
      "A simple approach worked well enough here. The idea was to calculate, for each country, the average points awarded to each other country. Coming from Slovenia which was once part of Yugoslavia, together with Croatia, Serbia, Bosnia and Herzegovina, Macedonia and Montenegro, it is perhaps not surprising that our voting patterns are rather interesting:\n",
      "  AVG' COUNTRY\n",
      "10.38  Serbia\n",
      " 8.53  Croatia\n",
      " 8.00  Bosnia and Herzegovina\n",
      " 5.91  Macedonia\n",
      " 3.21  Norway\n",
      " 3.17  Russia\n",
      " 3.07  Greece\n",
      "...\n",
      " 0.18  Portugal\n",
      " 0.17  Belarus\n",
      "\n",
      "It is painfully obvious that Slovenia is not judging the quality of the artist alone and it is well known that other countries follow similar patterns. It would, therefore, seem like a good idea to use this knowledge in predicting this year's voting.\n",
      "\n",
      "The estimated average points awarded are not very stable, especially for newer countries. To remedy this, instead of using:\n",
      "avg := sum(x) / |x|\n",
      "\n",
      "I used\n",
      "avg' := (sum(x) + 1) / (|x| + 1)\n",
      "\n",
      "The new estimate got better results on the cross-validation tests.\n",
      "\n",
      "Betting Odds\n",
      "\n",
      "Using just the voting patterns of countries to predict this year's results was not enough. I had to, somehow, incorporate the approximate betting odds as well. Many approaches could have worked well here. In the end I opted for the one that gave the best cross-validation results.\n",
      "\n",
      "I had to convert the approximate betting odds into something comparable with the average points awarded. I used:\n",
      "odds'(ctr) := 1 / log(odds(ctr)) * a + b\n",
      "\n",
      "The coefficients a and b were chosen experimentally, as the ones that gave the best cross-validation score.\n",
      "\n",
      "A small example will elucidate how I calculated the converted betting odds.\n",
      "odds'(Croatia) = 1 / log(odds(Croatia)) * 4.4 + 0.8 =\n",
      "               = 1 / log(48) * 4.4 + 0.8\n",
      "               = 1.94\n",
      "\n",
      "The converted betting odds for the top and bottom countries were:\n",
      "ODDS' COUNTRY\n",
      "5.23  Azerbaijan\n",
      "3.21  Germany\n",
      "2.54  Armenia\n",
      "...\n",
      "1.94  Croatia\n",
      "...\n",
      "1.45  Slovenia\n",
      "1.44  Bulgaria\n",
      "1.44  Macedonia\n",
      "1.44  Switzerland\n",
      "Combining the voting patterns with the betting odds\n",
      "\n",
      "It was now time to bring everything together. This was simply a matter of summing the average points awarded with the converted betting odds.\n",
      "\n",
      "This was how I predicted Slovenia's votes for this year:\n",
      "COUNTRY                  AVG'  ODDS'    SUM POINTS\n",
      "Serbia                 10.38 + 1.84 = 12.21     12\n",
      "Croatia                 8.53 + 1.94 = 10.47     10\n",
      "Bosnia and Herzegovina  8.00 + 1.49 =  9.49      8\n",
      "Macedonia               5.91 + 1.44 =  7.35      7\n",
      "Azerbaijan              1.80 + 5.23 =  7.03      6\n",
      "Norway                  3.21 + 2.01 =  5.22      5\n",
      "Greece                  3.07 + 1.96 =  5.03      4\n",
      "Sweden                  2.85 + 2.18 =  5.03      3\n",
      "Russia                  3.17 + 1.62 =  4.79      2\n",
      "Germany                 1.50 + 3.21 =  4.71      1\n",
      "Denmark                 2.42 + 2.25 =  4.66      0\n",
      "...\n",
      "\n",
      "We saw earlier that Slovenia's votes have little to do with song quality, as we usually award the top points to Balkan countries, no matter how bad they sing. The added betting odds should not influence the prediction of such countries considerably. On the other hand, if we take a country that is perhaps a bit more fair, like Israel, we see that the final predictions are affected to a greater extent:\n",
      "COUNTRY                  AVG'   ODDS'    SUM POINTS\n",
      "Armenia                 7.50 +  2.54 = 10.04     12\n",
      "Azerbaijan              3.75 +  5.23 =  8.98     10\n",
      "Russia                  7.23 +  1.62 =  8.85      8\n",
      "Ukraine                 6.30 +  1.54 =  7.84      7\n",
      "Romania                 6.07 +  1.61 =  7.68      6\n",
      "Greece                  4.08 +  1.96 =  6.04      5\n",
      "Georgia                 4.25 +  1.77 =  6.02      4\n",
      "Iceland                 3.83 +  1.72 =  5.55      3\n",
      "Serbia                  3.71 +  1.84 =  5.55      2\n",
      "Denmark                 3.25 +  2.25 =  5.50      1\n",
      "Sweden                  3.27 +  2.18 =  5.45      0\n",
      "...\n",
      "\n",
      "Cross validation\n",
      "The most important component of my solution was cross-validation and was probably the reason why I won the competition in the first place. It enabled me to try many different models and, between them, choose the one that was most likely to give the best results.\n",
      "The dataset was split into partitions, one for each Eurovision event. I then proceeded to build the model on all but one partition and calculated the error of that model on the partition that was left out. The procedure was repeated so that each time a different partition was left out. This gave me a fair estimate of how the model performs on unseen data.\n",
      "\n",
      "The cross-validation procedure in pseudocode:\n",
      "function crossValidation(dataset, buildModel):\n",
      "  error = 0\n",
      "  for year in eurovisionEvents:\n",
      "    learnData = {example | example in dataset and example.year != year}\n",
      "    testData  = {example | example in dataset and example.year == year}\n",
      "    model = buildModel(learnData)\n",
      "    error += testModel(model, testData)\n",
      "  return error\n",
      "Conclusion \n",
      "I am well aware that certain parts of my approach are not very strong. I had to do my best with the time that was available. I have many ideas for next year, which I will, for the moment at least, keep to myself :)\n",
      "I really enjoy competing in events like this and hope there will be more to come in the future.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Initial Strategy\n",
      "\n",
      "The graph shows both my public and private scores (which were obtained after the contest).  As you can see from the graph, my initial attempts were not very successful.   The training data contained 206 responders and 794 non- responders.  The test data was known to contain 346 of each.  I tried two separate to segmenting my training dataset:\n",
      "\n",
      "To make my training set closely match the  overall  population (32.6 % Responders) in order to accurately reflect  the  entire dataset.\n",
      "To make my training set closely match the test data in order to have a population similar to the test set.\n",
      "\n",
      "I   identified certain areas of the dataset that didn't appear to be   randomly partitioned. In order to do machine learning correctly, it is important to have your training data closely match the test dataset. I identified five   separate groups in the data which I began to treat separately.\n",
      "\n",
      "Originally   I set up a different model for each group, but that became a pain and I   found better results by simply estimating the overall group response   and adjusting the predictions in each group to match the predicted group   mean response.\n",
      "\n",
      "\n",
      "Matching Controls\n",
      "\n",
      "The group I  had designated “Yellow” [Patients 353:903] did have an average response  of 32.9% (close to the 32.6% overall dataset). I used the matchControls  function from the e1071 package in “R” to pick the best matches in the  “Yellow” group against the “Red” group (the majority of what needed to  be predicted).\n",
      "\n",
      "This allowed me to best match the features  VL.t0, CD4.t0, and rt184. These were the only three that at that time I  was confident were important, so I wanted to make sure they were  accurately represented.\n",
      "\n",
      "After a few more iterations  through match controls I was able to balance the “Yellow” data set to be  as close to the “Red” data set as possible – except for rt184. There  were further imbalances in the test data that were only resolved by  excluding the first 230 rows of the test data in some further  refinements.\n",
      "\n",
      "\n",
      "Recursive Feature Elimination via R 'caret' package\n",
      "\n",
      "I  felt I had now balanced out the training set as best I could in order  to then try to find more features that would predict patient response.\n",
      "\n",
      "I  attended the “’R’ User Conference 2010” in late July and saw a  presentation by Max Kuhn on the ‘caret’ package. I was unaware of this  package and it had many functions that looked interesting – particularly  the rfe function for feature selection.\n",
      "\n",
      "The rfe function  allowed me to quickly see what features were important. As each amino  acid was represented separately – I had over 600 features and this  obviously needed to be narrowed down.\n",
      "\n",
      "I ran this function countless times, but this is part of the actual output for my last submission:\n",
      "\n",
      "Variables Accuracy Kappa AccuracySD KappaSD Selected\n",
      "\n",
      "[rows omitted]\n",
      "\n",
      "90 0.7233 0.3148 0.04884 0.1121\n",
      "\n",
      "120 0.7383 0.3493 0.05648 0.1393 *\n",
      "\n",
      "150 0.7276 0.3225 0.04698 0.1153\n",
      "\n",
      "[rows omitted]\n",
      "\n",
      "The top 5 variables (out of 120):\n",
      "\n",
      "VL.t0, QIYQEPFKNLK, rt184, CD4.t0, rt215\n",
      "\n",
      "The  last line shows you the five judged most important. The rfe function  has selected 120 variables as being optimum, but I went for a smaller  amount for various reasons. What was most impressive to me is that off  the five variables shown here – rt184 and rt215 are both listed. I  didn’t have time to do much research on the topic, but I had read  several papers that had all mentioned rt184 as being important and rt215  was probably the second or third most mentioned RT codon in the few  papers I read.\n",
      "\n",
      "Training via R 'caret' and ‘randomForest’ packages\n",
      "\n",
      "I  trained my models and made my predictions using the randomForest  function both alone and with some tuning and validation enhancements  from the caret package using variables I had selected in the previous  step. I would highly recommend the caret package to anyone using “R” for  machine learning. I enjoyed this contest immensely and look forward to  some free time to work on the Chess contest.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "About me:\n",
      "I’m an embedded systems engineer, currently working for a small engineering company in Las Cruces, New Mexico. I graduated from New Mexico Tech in 2007, with degrees in Electrical Engineering and Computer Science. Like many people, I first became interested in algorithm competitions with the Netflix Prize a few years ago. I was quite excited to find the Kaggle site a few months ago, as I enjoy participating in these types of competitions.\n",
      "\n",
      "Explanation of Technique:\n",
      "Though I tried several different methods, I used a weighted combination of three predictors to come up with the final forecast.\n",
      "\n",
      "\n",
      "\n",
      "#1: After reviewing Athanasopoulos et al, it became obvious that the naive predictor was a good algorithm with which to start. It is both easy to implement and performed well when compared to the other algorithms in the paper.\n",
      "\n",
      "After graphing a few of the time series, it became apparent that many of the series increase with time. Indeed, the second sentence of the Athanasopoulos paper states that globally tourism has grown “at a rate of 6% annually.” In order to take advantage of this knowledge, I multiplied the  (Naive algorithm’s) predicted value by a factor to take this growth into account. With some testing, I determined a 5.5% growth factor to yield the lowest MASE.\n",
      "\n",
      "prediction1 = last_value * (1.055 ** number_of_years_in_the_future)\n",
      "\n",
      "#2: I examined fitting a polynomial line to the data and using the line to predict future values. I tried using first through fifth order polynomials to find that the lowest MASE was obtained using a first order polynomial (simple regression line). This best fit line was used to predict future values. I also kept the r**2 value of the fit for use in blending the results of the predictor.\n",
      "\n",
      "#3: In thinking about these two predictors, I recognized that the naive predictor, though accurate, throws away most of the provided data, and only uses a single element of the time series. The polynomial line predictor uses all of the data, weighted equally, though the most recent data is probably of more value in indicating future performance than the earlier data in the time series. I examined and eventually used an exponentially-weighted least squares regression to fit a line to the data. This algorithm gave more accurate predictions for many of the time series by itself, and also lowered the MASE when used in combination with the two above predictors. The r**2 value of this fit was also used for blending the predictors.\n",
      "Blending stage:\n",
      "I started with a basic weighted blend of predictors. I used a constant weight factor for the modified naive predictor, while blending weights for the unweighted and weighted regression lines depending on the r**2 values found in fitting the time series. I selected the logistic function as a way of gradually increasing the weight with increasing r**2 value:\n",
      "\n",
      "weight = a * logistic( b * (r**2 - c))\n",
      "\n",
      "Values for a, b, and c were determined by trial and error. I also examined using some numeric optimization functions included in Python to minimize a training set MASE. While this succeeded in lower the training set MASE, I discarded this method when I received a lower leaderboard MASE (possibly from overfitting).\n",
      "\n",
      "Testing / developing algorithms:\n",
      "When testing algorithms, I used the last four years of the training dataset (after removing them from the data I was using for training) to test against. While this worked well in the initial stages, I found that once my leaderboard MASE got below about 2.05, this ‘training MASE’ became a much less reliable indicator of whether the leaderboard MASE would increase or decrease with a change. So, during the last few weeks of the contest, I primarily made small tweaks, and tested their value by submitting a new prediction to Kaggle rather than comparing my MASE results. I believe that this indicates a significant difference in nature of the last 4 years of the training set compared to the 4 years in the test set. If the test set includes data from 2008-2009, I’m speculating that depressed tourism numbers as a result of the global economic recession could have caused a significant difference in the trends.\n",
      "\n",
      "Possible improvements:\n",
      "While the above method seemed to work fairly well at predicting tourism numbers, there are several steps that could have likely improved the score. I only implemented the Naive method implemented in the Athanasopoulos paper; I do think that including a couple of other algorithms’ output into the final blend could have further increased the score. If I had a few more days to work on a solution, I would have tried to implement the Theta and ARIMA methods described in the paper and looked at the effects of including them in the blend.\n",
      "\n",
      "I think an investigation into how to come up with a blending method that doesn’t use as much manual tweaking would also be of benefit.\n",
      "\n",
      "I enjoyed participating in part one, and look forward to part two of the contest.\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "html_parsed = []\n",
    "\n",
    "for i in range(0, 3):\n",
    "    html_parsed.append(BeautifulSoup(df.loc[i, 'content'], 'html.parser'))\n",
    "\n",
    "for i in html_parsed:\n",
    "    print(i.get_text())\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b00c450",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
